{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we predict the category of an app from the app's description?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to see if we can create a Machine Learning model that will accuretly predict the category of an app from the app's description. To do this we created a web scraper to scrape data from the GooglePlay Store. After cleaning our data, we performed NLTK, Feature Engineering and Model Fitting to create an optimal ML Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a web scraper to collect 60 app descriptions per category for 18 categories. After collecting our data we saved it to a file in dictionary format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "read_dictionary = np.load('my_file.npy').item()\n",
    "\n",
    "data  = read_dictionary\n",
    "\n",
    "print(data.keys())\n",
    "data['EDUCATION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK: Natural Language Tool Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# stem words\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "\n",
    "def text_cleaner(description):\n",
    "    '''uses regex to tokenize words and capture them from the description, \n",
    "    lowers the capitilization remove stop words, reduce to stem words, \n",
    "    and joins them all in a string'''\n",
    "    tokens_raw = nltk.regexp_tokenize(description, pattern)\n",
    "    tokens = [i.lower() for i in tokens_raw]\n",
    "    tokens_stopped = [w for w in tokens if not w in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in tokens_stopped]\n",
    "    cleaned = ' '.join(stemmed)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "def dict_cleaner(dictionary):\n",
    "    '''iterates through the dictionary values in each key (category) \n",
    "    and cleans each description and adds it back to a new list'''\n",
    "    description_list = []\n",
    "    for c, d in dictionary.items():\n",
    "        for description in d:\n",
    "            cleaned = text_cleaner(description)\n",
    "            description_list.append(cleaned)\n",
    "    return description_list\n",
    "\n",
    "\n",
    "#use our function on our data\n",
    "description_list = dict_cleaner(data)\n",
    "description_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Tf-Idif function to transform our data to reflect how important a word is in the collection of descriptions in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tfidf.fit_transform(description_list)\n",
    "\n",
    "df = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many non-zero elements are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_cols = response.nnz / float(response.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Reviews: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(response.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Train-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We have to create labels in order to label the words that appear in each category\n",
    "def create_labels(dictionary):\n",
    "    x = dictionary.keys()\n",
    "    new_list = []\n",
    "    for c in x: \n",
    "        s = [c] * len(dictionary[c])\n",
    "        new_list += s\n",
    "    return new_list\n",
    "\n",
    "\n",
    "labels = create_labels(data)\n",
    "df['labels'] = labels\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we set our labels to y and set our features to x\n",
    "y = df.labels\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "\n",
    "#set our train and test data\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "knn_train_preds = knn.predict(X_train)\n",
    "knn_test_preds = knn.predict(X_test)\n",
    "\n",
    "knn_train_score = accuracy_score(y_train, knn_train_preds)\n",
    "knn_test_score = accuracy_score(y_test, knn_test_preds)\n",
    "\n",
    "print(\"KNN\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(knn_train_score, knn_test_score))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, knn_test_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive-Bayes Multinomial Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "nb_train_preds = nb_classifier.predict(X_train)\n",
    "nb_test_preds = nb_classifier.predict(X_test)\n",
    "\n",
    "nb_train_score = accuracy_score(y_train, nb_train_preds)\n",
    "nb_test_score = accuracy_score(y_test, nb_test_preds)\n",
    "\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, nb_test_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_train_preds = rf_classifier.predict(X_train)\n",
    "rf_test_preds = rf_classifier.predict(X_test)\n",
    "\n",
    "rf_train_score = accuracy_score(y_train, rf_train_preds)\n",
    "rf_test_score = accuracy_score(y_test, rf_test_preds)\n",
    "\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score, rf_test_score))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, rf_test_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_clf = svm.SVC(probability=True)\n",
    "\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_train_preds = rf_classifier.predict(X_train)\n",
    "svm_test_preds = rf_classifier.predict(X_test)\n",
    "\n",
    "svm_train_score = accuracy_score(y_train, svm_train_preds)\n",
    "svm_test_score = accuracy_score(y_test, svm_test_preds)\n",
    "\n",
    "print('SVM')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(svm_train_score, svm_test_score))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, svm_test_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT for automated model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, cv = 3 ,population_size=20,\\\n",
    "                      max_eval_time_mins=10, verbosity=3)\n",
    "\n",
    "\n",
    "#we ran the classifier, which will tell us the best model to use.\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "'''After changing the generation and population parameters to get better results, \n",
    "   we came up with the final best result. \n",
    "   The Automated model selection gave us the best model to use which yielded: \n",
    "   \n",
    "   \n",
    "   exported_pipeline = LinearSVC(C=1, dual=True, loss=\"squared_hinge\", penalty=\"l2\")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc_classifier = LinearSVC(C=10.0, dual=False, loss=\"squared_hinge\", penalty=\"l2\", tol=0.1)\n",
    "\n",
    "lsvc_classifier.fit(X_train, y_train)\n",
    "lsvc_train_preds = lsvc_classifier.predict(X_train)\n",
    "lsvc_test_preds = lsvc_classifier.predict(X_test)\n",
    "\n",
    "lsvc_train_score = accuracy_score(y_train, lsvc_train_preds)\n",
    "lsvc_test_score = accuracy_score(y_test, lsvc_test_preds)\n",
    "\n",
    "print('LinearSVC')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(lsvc_train_score, lsvc_test_score))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, lsvc_test_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(learning_rate=0.01, max_depth=6, max_features=0.2, min_samples_leaf=3, min_samples_split=15, n_estimators=100, subsample=0.25)\n",
    "\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_train_preds = gb_clf.predict(X_train)\n",
    "gb_test_preds = gb_clf.predict(X_test)\n",
    "\n",
    "gb_train_score = accuracy_score(y_train, gb_train_preds)\n",
    "gb_test_score = accuracy_score(y_test, gb_test_preds)\n",
    "\n",
    "print('Gradient Boosting')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(gb_train_score, gb_test_score))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, gb_test_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier()\n",
    "\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "adaboost_train_preds = adaboost_clf.predict(X_train)\n",
    "adaboost_test_preds = adaboost_clf.predict(X_test)\n",
    "\n",
    "adaboost_train_score = accuracy_score(y_train, adaboost_train_preds)\n",
    "adaboost_test_score = accuracy_score(y_test, adaboost_test_preds)\n",
    "\n",
    "print('AdaBoosting')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(adaboost_train_score, adaboost_test_score))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, adaboost_test_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_train_preds = xgb_clf.predict(X_train)\n",
    "xgb_test_preds = xgb_clf.predict(X_test)\n",
    "\n",
    "xgb_train_score = accuracy_score(y_train, xgb_train_preds)\n",
    "xgb_test_score = accuracy_score(y_test, xgb_test_preds)\n",
    "\n",
    "print('AdaBoosting')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(xgb_train_score, xgb_test_score))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, xgb_test_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "svc1 = LinearSVC(C=25.0, dual=False, loss=\"squared_hinge\", penalty=\"l2\", tol=0.001)\n",
    "svc2 = LinearSVC(C=1.0, dual=True, loss=\"hinge\", penalty=\"l2\", tol=1e-05)\n",
    "\n",
    "\n",
    "\n",
    "vc_clf = VotingClassifier(estimators=[('svc1', svc1), \n",
    "                                     ('svc2', svc2),\n",
    "                                     ('gb_clf', gb_clf),\n",
    "                                    ('nb', nb_classifier),\n",
    "                                    ('knn', knn),\n",
    "                                    ('rf', rf_classifier)], voting='hard')\n",
    "\n",
    "vc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_train_preds = vc_clf.predict(X_train)\n",
    "vc_test_preds = vc_clf.predict(X_test)\n",
    "\n",
    "vc_train_score = accuracy_score(y_train, vc_train_preds)\n",
    "vc_test_score = accuracy_score(y_test, vc_test_preds)\n",
    "\n",
    "print('Voting Classifier')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(vc_train_score, vc_test_score))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, vc_test_preds, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(.95)\n",
    "pca.n_components_\n",
    "\n",
    "pca_train = pca.fit_transform(X_train)\n",
    "pca_test = pca.transform(X_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
