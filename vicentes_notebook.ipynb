{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saves dictionary in same format\n",
    "# np.save('my_file.npy', all_desc) \n",
    "\n",
    "# # # Loads dictionary in same format so we don't have to webscrape all the time\n",
    "read_dictionary = np.load('my_file.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu  = read_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['EDUCATION', 'BUSINESS', 'DATING', 'SPORTS', 'WEATHER', 'FOOD_AND_DRINK', 'HEALTH_AND_FITNESS', 'BEAUTY', 'MUSIC_AND_AUDIO', 'NEWS_AND_MAGAZINES', 'SOCIAL', 'SHOPPING', 'PRODUCTIVITY', 'PHOTOGRAPHY', 'MEDICAL', 'PARENTING', 'COMMUNICATION', 'TOOLS'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edu.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patter to capture using regex\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem words\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(description):\n",
    "    ''''''\n",
    "    tokens_raw = nltk.regexp_tokenize(description, pattern)\n",
    "    tokens = [i.lower() for i in tokens_raw]\n",
    "    tokens_stopped = [w for w in tokens if not w in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in tokens_stopped]\n",
    "    cleaned = ' '.join(stemmed)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_cleaner(dictionary):\n",
    "    description_list = []\n",
    "    for c, d in dictionary.items():\n",
    "        for description in d:\n",
    "            cleaned = text_cleaner(description)\n",
    "            description_list.append(cleaned)\n",
    "    return description_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description list is a list of all descriptions stemmed and in order\n",
    "description_list = dict_cleaner(edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(description_list, columns=['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we had exactly 60 items in each category this creates a list\n",
    "# with 60 items of each key.\n",
    "def create_labels(dictionary):\n",
    "    x = dictionary.keys()\n",
    "    new_list = []\n",
    "    for c in x: \n",
    "        s = [c] * len(dictionary[c])\n",
    "        new_list += s\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = create_labels(edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es una app genial porqu aprend palabra nueva p...</td>\n",
       "      <td>EDUCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unlock math calcul math solut take photo math ...</td>\n",
       "      <td>EDUCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learn anyth free forev spend afternoon brush s...</td>\n",
       "      <td>EDUCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>updat info latest updat new section ad learn l...</td>\n",
       "      <td>EDUCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>join million learner learn code make app game ...</td>\n",
       "      <td>EDUCATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                desc     labels\n",
       "0  es una app genial porqu aprend palabra nueva p...  EDUCATION\n",
       "1  unlock math calcul math solut take photo math ...  EDUCATION\n",
       "2  learn anyth free forev spend afternoon brush s...  EDUCATION\n",
       "3  updat info latest updat new section ad learn l...  EDUCATION\n",
       "4  join million learner learn code make app game ...  EDUCATION"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "listy = []\n",
    "for key in edu.keys():\n",
    "    string = ''\n",
    "    for x in range(i, i + 60):\n",
    "        string += description_list[i]\n",
    "    listy.append(string)\n",
    "    i += 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common(keys, lis_t, n_words=50):\n",
    "    key_words = []\n",
    "    for key, string in zip(keys, lis_t):\n",
    "        meta_freqdist = FreqDist(string.split())\n",
    "        key_words.append((key, meta_freqdist.most_common(n_words)))\n",
    "    return key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = get_most_common(edu.keys(), listy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs words vs frequency\n",
    "# meta_freqdist.plot(30,cumulative=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building wordclouds based on key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_cloud(tuppple):\n",
    "    words = [x[0] for x in tuppple[1]]\n",
    "    colors = [plotly.colors.DEFAULT_PLOTLY_COLORS[random.randrange(1, 10)] for i in range(30)]\n",
    "\n",
    "    weights_ = []\n",
    "    for x in tuppple[1]:\n",
    "        if x[1] >= 5:\n",
    "            weights_.append(round(x[1]/5))\n",
    "        else:\n",
    "            weights_.append(x[1])\n",
    "#     scaler = MinMaxScaler()\n",
    "#     weights_ = scaler.fit_transform(np.array([x[1] for x in tuppple[1]]).reshape(-1,1))\n",
    "#     y = weights_.flatten()*10\n",
    "#     weights = y.round().astype(int)\n",
    "    weights = weights_\n",
    "\n",
    "    data = go.Scatter(x=[random.random() for i in range(30)],\n",
    "                     y=[random.random() for i in range(30)],\n",
    "                     mode='text',\n",
    "                     text=words,\n",
    "                     marker={'opacity': 0.3},\n",
    "                     textfont={'size': weights,\n",
    "                               'color': colors})\n",
    "    layout = go.Layout({ 'title' : f'{tuppple[0]}',\n",
    "                        'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n",
    "                        'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})\n",
    "    fig = go.Figure(data=[data], layout=layout)\n",
    "\n",
    "    plot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through topics and displays word clouds for each topic every 3 seconds\n",
    "for topic in key_words:\n",
    "    build_word_cloud(topic)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['desc', 'labels'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_tr, X_te, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tfidf.fit_transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Reviews: 112.73032407407408\n",
      "Percentage of columns containing 0: 0.9903360202251115\n"
     ]
    }
   ],
   "source": [
    "non_zero_cols = response.nnz / float(response.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Reviews: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(response.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVE FOR MODELS AND OTHERS SHOWING PREFORMANCE, <- CANT DO... \n",
    "# TEXT RELATED VIZUALISATIONS LIKE WORD CLOUD, LATENT SEMANTIC ANALYSIS\n",
    "# FEATURE IMPORTANCE ON RF, NB\n",
    "# CREATE VIZ OF ENSEMBLE MODELS\n",
    "# TYING SPECIFIC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT for automated model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-2edae0f8547f>, line 129)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-2edae0f8547f>\"\u001b[0;36m, line \u001b[0;32m129\u001b[0m\n\u001b[0;31m    }x\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "classifier_config_dict = {\n",
    "\n",
    "    # Classifiers\n",
    "\n",
    "    'sklearn.ensemble.GradientBoostingClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'max_depth': range(1, 11),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.neighbors.KNeighborsClassifier': {\n",
    "        'n_neighbors': range(1, 101),\n",
    "        'weights': [\"uniform\", \"distance\"],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "\n",
    "    'sklearn.svm.LinearSVC': {\n",
    "        'penalty': [\"l1\", \"l2\"],\n",
    "        'loss': [\"hinge\", \"squared_hinge\"],\n",
    "        'dual': [True, False],\n",
    "        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]\n",
    "    },\n",
    "\n",
    "    'xgboost.XGBClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': range(1, 11),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'nthread': [1]\n",
    "    },\n",
    "\n",
    "    # Preprocesssors\n",
    "    'sklearn.preprocessing.Binarizer': {\n",
    "        'threshold': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.decomposition.FastICA': {\n",
    "        'tol': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.cluster.FeatureAgglomeration': {\n",
    "        'linkage': ['ward', 'complete', 'average'],\n",
    "        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.Nystroem': {\n",
    "        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05),\n",
    "        'n_components': range(1, 11)\n",
    "    },\n",
    "\n",
    "    'sklearn.decomposition.PCA': {\n",
    "        'svd_solver': ['randomized'],\n",
    "        'iterated_power': range(1, 11)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.PolynomialFeatures': {\n",
    "        'degree': [2],\n",
    "        'include_bias': [False],\n",
    "        'interaction_only': [False]\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.RBFSampler': {\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.RobustScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.StandardScaler': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.ZeroCount': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.OneHotEncoder': {\n",
    "        'minimum_fraction': [0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "        'sparse': [False],\n",
    "        'threshold': [10]\n",
    "    },\n",
    "\n",
    "    # Selectors\n",
    "    'sklearn.feature_selection.SelectFwe': {\n",
    "        'alpha': np.arange(0, 0.05, 0.001),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectPercentile': {\n",
    "        'percentile': range(1, 100),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.VarianceThreshold': {\n",
    "        'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.RFE': {\n",
    "        'step': np.arange(0.05, 1.01, 0.05),\n",
    "        'estimator': {\n",
    "            'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "                'n_estimators': [100],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectFromModel': {\n",
    "        'threshold': np.arange(0, 1.01, 0.05),\n",
    "        'estimator': {\n",
    "            'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "                'n_estimators': [100],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Changed generation and population parameters to get better results\n",
    "tpot = TPOTClassifier(generations=5, cv = 3 ,population_size=10,\\\n",
    "                      max_eval_time_mins=10,config_dict=classifier_config_dict ,verbosity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 operators have been imported by TPOT.\n",
      "Warning: Since the input matrix is a sparse matrix, please makes sure all the operators in the customized config dictionary supports sparse matriies.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=60, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped pipeline #14 due to time out. Continuing to the next pipeline.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.6898619237187607\tKNeighborsClassifier(input_matrix, KNeighborsClassifier__n_neighbors=24, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.6898619237187607\tKNeighborsClassifier(input_matrix, KNeighborsClassifier__n_neighbors=24, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.6898619237187607\tKNeighborsClassifier(input_matrix, KNeighborsClassifier__n_neighbors=24, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #45 due to time out. Continuing to the next pipeline.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.7476731636184105\tLinearSVC(input_matrix, LinearSVC__C=0.1, LinearSVC__dual=False, LinearSVC__loss=squared_hinge, LinearSVC__penalty=l2, LinearSVC__tol=0.001)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.7476731636184105\tLinearSVC(input_matrix, LinearSVC__C=0.1, LinearSVC__dual=False, LinearSVC__loss=squared_hinge, LinearSVC__penalty=l2, LinearSVC__tol=0.001)\n",
      "\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict={'sklearn.ensemble.GradientBoostingClassifier': {'n_estimators': [100], 'learning_rate': [0.001, 0.01, 0.1, 0.5, 1.0], 'max_depth': range(1, 11), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'subsample': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45,....3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}}}},\n",
       "        crossover_rate=0.1, cv=3, disable_update_check=False,\n",
       "        early_stop=None, generations=5, max_eval_time_mins=10,\n",
       "        max_time_mins=None, memory=None, mutation_rate=0.9, n_jobs=1,\n",
       "        offspring_size=None, periodic_checkpoint_folder=None,\n",
       "        population_size=10, random_state=None, scoring=None, subsample=1.0,\n",
       "        use_dask=False, verbosity=3, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.export('class5-pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8009259259259259 First pipeline trial\n",
    "# 0.8055555555555556 Second pipeline trial\n",
    "# 0.7685185185185185 Third Pipeline trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class5_clf = LinearSVC(C=0.1, dual=False, loss=\"squared_hinge\", penalty=\"l2\", tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class5_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8101851851851852"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class5_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            BEAUTY       0.86      0.75      0.80        16\n",
      "          BUSINESS       0.80      0.80      0.80        10\n",
      "     COMMUNICATION       0.58      0.58      0.58        12\n",
      "            DATING       0.57      1.00      0.73         4\n",
      "         EDUCATION       0.88      0.64      0.74        11\n",
      "    FOOD_AND_DRINK       1.00      0.83      0.91        12\n",
      "HEALTH_AND_FITNESS       0.83      0.91      0.87        11\n",
      "           MEDICAL       1.00      0.82      0.90        17\n",
      "   MUSIC_AND_AUDIO       0.88      0.93      0.90        15\n",
      "NEWS_AND_MAGAZINES       0.80      0.89      0.84         9\n",
      "         PARENTING       0.67      0.91      0.77        11\n",
      "       PHOTOGRAPHY       0.78      0.88      0.82        16\n",
      "      PRODUCTIVITY       0.44      0.57      0.50         7\n",
      "          SHOPPING       0.82      1.00      0.90        14\n",
      "            SOCIAL       0.73      0.57      0.64        14\n",
      "            SPORTS       1.00      1.00      1.00        11\n",
      "             TOOLS       0.78      0.54      0.64        13\n",
      "           WEATHER       1.00      1.00      1.00        13\n",
      "\n",
      "         micro avg       0.81      0.81      0.81       216\n",
      "         macro avg       0.80      0.81      0.80       216\n",
      "      weighted avg       0.82      0.81      0.81       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, class5_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = LinearSVC(C=10.0, dual=False, loss=\"squared_hinge\", penalty=\"l2\", tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc1 = LinearSVC(C=25.0, dual=False, loss=\"squared_hinge\", penalty=\"l2\", tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2 = LinearSVC(C=1.0, dual=True, loss=\"hinge\", penalty=\"l2\", tol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier(learning_rate=0.01, max_depth=6, max_features=0.2, min_samples_leaf=3, min_samples_split=15, n_estimators=100, subsample=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VotingClassifier(estimators=[('svc1', svc1), \n",
    "                                     ('svc2', svc2),\n",
    "                                     ('gb_clf', gb_clf),\n",
    "                                    ('nb', nb_classifier),\n",
    "                                    ('knn', knn),\n",
    "                                    ('rf', rf_classifier)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 with: svc1, svc2, last, gb_clf, nb, knn, rf\n",
    "# model2 with: svc1, svc2, gb_clf, nb, knn, rf\n",
    "# last -> SVC model \n",
    "#         LinearSVC(C=10.0, dual=False, loss=\"squared_hinge\", penalty=\"l2\", tol=0.1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# model1 => 0.8101851851851852 <- voting\n",
    "# model2 => 0.8148148148148148 <-voting\n",
    "# last => 0.8194444444444444 <-svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Store Models:\n",
    "# joblib.dump(model, 'model2')\n",
    "\n",
    "\n",
    "# To load model\n",
    "# model1 = joblib.load('model1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive-Bayes Multinomial Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier.fit(X_train, y_train)\n",
    "nb_train_preds = nb_classifier.predict(X_train)\n",
    "nb_test_preds = nb_classifier.predict(X_test)\n",
    "\n",
    "nb_train_score = accuracy_score(y_train, nb_train_preds)\n",
    "nb_test_score = accuracy_score(y_test, nb_test_preds)\n",
    "\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_train_preds = rf_classifier.predict(X_train)\n",
    "rf_test_preds = rf_classifier.predict(X_test)\n",
    "\n",
    "rf_train_score = accuracy_score(y_train, rf_train_preds)\n",
    "rf_test_score = accuracy_score(y_test, rf_test_preds)\n",
    "\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score, rf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pca.fit_transform(X_train)\n",
    "new_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.fit(new_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('pca2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline = ExtraTreesClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.5, min_samples_leaf=3, min_samples_split=7, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline.fit(new_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline.score(new_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last.fit(new_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last.score(new_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    #Add Normalization Option\n",
    "    '''prints pretty confusion metric with normalization option '''\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = last.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate = edu.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING THING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_desc = \"\"\"Find your next commercial real estate deal on the go! Leverage the power of LoopNet anytime, anywhere to find your perfect investment property or lease space right from your Android device. Whether in the office or on the road, the LoopNet app keeps you productive, informed, and ahead of the competition with these features:\n",
    "\n",
    "• Advanced Search Filters: Find the properties that meet your exact needs using specific search criteria and filtering options.\n",
    "• Comprehensive Property Details: View listing details including property data, photos, videos, financials, map location, satellite images, and street view. See something you like? Contact the broker right from your phone!\n",
    "• Map or List Based Search: Customize your search experience. Display and adjust your search results on a map, a list, or both.\n",
    "• Saved Searches & Notifications: Save searches that matter so you can access them quickly and monitor your market day-to-day. We’ll also keep you notified about new listings that match your saved search criteria.\n",
    "• Watch List & Notifications: Monitor listings of interest by adding them to your Watch List. We’ll notify you as soon as there are any changes or updates to the listings.\n",
    "• Multiple Device Capability: Access your saved searches and watched listings from any device or on the web through your LoopNet account.\n",
    "\n",
    "LoopNet is the industry leader with more traffic, listings, and geographic coverage than any other commercial real estate marketplace. Use the LoopNet app to find your next investment property or space to rent, whether office space, multifamily apartment buildings, retail, restaurant, medical, industrial, or land in all major markets in the United States and Canada including Los Angeles, New York, Chicago, San Francisco, Dallas, Houston, and Miami. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trial = text_cleaner(new_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vect = tfidf.transform([x_trial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trial[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffff = pd.DataFrame(x_vect.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
